{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### This notebook is designed to run on collab (for GPU). \n",
        "\n",
        "It contains training of Cycle GANs and Multi-Cycle GAN, DINOV2 features extraction and potential fine-tunnig, MedImageInsight features extraction and training + inference on the test set with the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA0arliIDMe6",
        "outputId": "4e49ad01-9830-47db-d4e2-5d9042625f22"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/MANY09F4/kaggle-DL-MI.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2gyfI3sPeLk",
        "outputId": "b74e068d-fb8c-4015-b5b5-5bd6a82c136a"
      },
      "outputs": [],
      "source": [
        "# We get the data from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jutlB9ImPq8c",
        "outputId": "449f1f11-1eff-435a-bed1-db7a89126b80"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFLgol1DQc3J",
        "outputId": "d344cd31-3a31-456c-c341-22dbf25e2d37"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb2YclztSMQO",
        "outputId": "3f27fbdf-d150-4630-e2a1-eaf822d3802f"
      },
      "outputs": [],
      "source": [
        "!pip install -r kaggle-DL-MI/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7Gz5Lc9cQXR",
        "outputId": "e30735c5-12f8-48bf-df15-48ee85a6ecac"
      },
      "outputs": [],
      "source": [
        "%cd /content/kaggle-DL-MI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfGLL4pU_G9A",
        "outputId": "299dd6fa-35ee-4a93-d55d-3f4a05628949"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import torch\n",
        "\n",
        "# List to store IDs of aberrant (corrupted) images in the training set\n",
        "list_aberrant_ids_train = []\n",
        "\n",
        "# Open the training HDF5 file\n",
        "with h5py.File('/content/drive/MyDrive/kaggle-DL-MI/data/train.h5', 'r') as f:\n",
        "    img_ids = list(f.keys())  # Get all image IDs\n",
        "\n",
        "    # Iterate through all image IDs\n",
        "    for img_id in img_ids:\n",
        "        # Load the image as a float tensor\n",
        "        img = torch.tensor(f[img_id]['img'][()]).float()\n",
        "\n",
        "        # Count how many pixels are exactly zero\n",
        "        if (img == 0).sum().item() > 200:\n",
        "            # If too many black pixels, mark image as aberrant\n",
        "            print(f\"Aberrant image found at ID: {img_id} with 0s in the image {(img == 0).sum().item()}\")\n",
        "            list_aberrant_ids_train.append(img_id)\n",
        "\n",
        "# Summary: number of aberrant images detected\n",
        "print(f\"Number of aberrant images in train set: {len(list_aberrant_ids_train)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uz6FasI_Kgz",
        "outputId": "7fd4626f-9db7-454b-834c-f2dbaa5d5536"
      },
      "outputs": [],
      "source": [
        "# List to store IDs of aberrant (corrupted) images in the validation set\n",
        "list_aberrant_ids_val = []\n",
        "\n",
        "# Open the validation HDF5 file\n",
        "with h5py.File('/content/drive/MyDrive/kaggle-DL-MI/data/val.h5', 'r') as f:\n",
        "    img_ids = list(f.keys())  # Get all image IDs\n",
        "\n",
        "    # Iterate through all images\n",
        "    for img_id in img_ids:\n",
        "        # Load the image as a float tensor\n",
        "        img = torch.tensor(f[img_id]['img'][()]).float()\n",
        "\n",
        "        # Check if the image contains more than 200 black pixels (value = 0)\n",
        "        if (img == 0).sum().item() > 200:\n",
        "            print(f\"Aberrant image found at ID: {img_id} with 0s in the image\")\n",
        "            list_aberrant_ids_val.append(img_id)\n",
        "\n",
        "# Print the total number of aberrant images detected in validation set\n",
        "print(f\"Number of aberrant images in val set: {len(list_aberrant_ids_val)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "P4OQhoyz_Phw",
        "outputId": "9f527acb-7f37-4e1a-bab9-a4eced4df1f4"
      },
      "outputs": [],
      "source": [
        "# List to store IDs of aberrant (corrupted) images in the test set\n",
        "list_aberrant_ids_test = []\n",
        "\n",
        "# Open the test HDF5 file\n",
        "with h5py.File('/content/drive/MyDrive/kaggle-DL-MI/data/test.h5', 'r') as f:\n",
        "    img_ids = list(f.keys())  # Get all image IDs\n",
        "\n",
        "    # Iterate through all images\n",
        "    for img_id in img_ids:\n",
        "        # Load the image as a float tensor\n",
        "        img = torch.tensor(f[img_id]['img'][()]).float()\n",
        "\n",
        "        # Check if the image contains more than 200 black pixels (value = 0)\n",
        "        if (img == 0).sum().item() > 200:\n",
        "            print(f\"Aberrant image found at ID: {img_id} with 0s in the image\")\n",
        "            list_aberrant_ids_test.append(img_id)\n",
        "\n",
        "# Print the total number of aberrant images detected in test set\n",
        "print(f\"Number of aberrant images in test set: {len(list_aberrant_ids_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvSbwQxoFYny",
        "outputId": "eda72b14-6ed2-481b-ef69-b8fca39dcfe0"
      },
      "outputs": [],
      "source": [
        "# Combine the aberrant image IDs from both training and validation sets\n",
        "list_aberrant_ids = list_aberrant_ids_train + list_aberrant_ids_val\n",
        "\n",
        "# Print the total number of unique aberrant images from both sets\n",
        "len(list_aberrant_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "M3bIdnUgGUrx",
        "outputId": "3f30ab65-614a-4bd8-9031-fbec8f0e97bd"
      },
      "outputs": [],
      "source": [
        "# Convert the lists of aberrant image IDs (training and val sets) into a comma-separated strings\n",
        "str_aberrant_ids_train = \",\".join(map(str, list_aberrant_ids_train))\n",
        "str_aberrant_ids_val = \",\".join(map(str, list_aberrant_ids_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZI1-vwg3T_0"
      },
      "source": [
        "## Mutli-Cycle GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tcx0wp8J3St0",
        "outputId": "83743199-55ce-4965-bf76-3915c4e7e65c"
      },
      "outputs": [],
      "source": [
        "# Launch training of the MultiStain-CycleGAN model with all source centers\n",
        "# Source domains = train + val / Target = test\n",
        "# No color augmentation is applied (values = 0)\n",
        "# Discriminator thresholding is enabled (D_thresh = 0.1)\n",
        "# Aberrant images from train/val are excluded\n",
        "\n",
        "!python -m CycleGAN.train_CycleGAN \\\n",
        "  --train_path \"/content/drive/MyDrive/kaggle-DL-MI/data/train.h5\" \\\n",
        "  --val_path \"/content/drive/MyDrive/kaggle-DL-MI/data/val.h5\" \\\n",
        "  --test_path \"/content/drive/MyDrive/kaggle-DL-MI/data/test.h5\" \\\n",
        "  --name multistain_run_all_domains \\\n",
        "  --batch_size 64 \\\n",
        "  --gpu_ids 0 \\\n",
        "  --n_epochs 7 \\\n",
        "  --n_epochs_decay 3 \\\n",
        "  --lr_G 0.0002 \\\n",
        "  --lr_D 0.0002 \\\n",
        "  --save_epoch_freq 2 \\\n",
        "  --display_id 0 \\\n",
        "  --lambda_A 10.0 \\\n",
        "  --lambda_B 10.0 \\\n",
        "  --D_thresh \\\n",
        "  --D_thresh_value 0.1 \\\n",
        "  --lambda_identity 0.5 \\\n",
        "  --gan_mode lsgan \\\n",
        "  --color_augment \\\n",
        "  --brightness 0 \\\n",
        "  --contrast 0 \\\n",
        "  --saturation 0 \\\n",
        "  --hue 0 \\\n",
        "  --aberrant_ids_train \"$str_aberrant_ids_train\" \\\n",
        "  --aberrant_ids_val \"$str_aberrant_ids_val\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2SrH0xpKJemU",
        "outputId": "df723937-63b1-4a53-e9bb-dd6eab9a9379"
      },
      "outputs": [],
      "source": [
        "# Zip and download model checkpoints from Colab\n",
        "# Compress the 'checkpoints' directory (containing the saved GAN weights)\n",
        "!zip -r /content/checkpoints_multi_domains.zip /content/kaggle-DL-MI/checkpoints\n",
        "\n",
        "# Trigger download to your local machine\n",
        "from google.colab import files\n",
        "files.download('/content/checkpoints_multi_domains.zip')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHMlSP273Z96"
      },
      "source": [
        "## Unique-Cycle GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHGOLSSlec_N",
        "outputId": "bd6199a8-a67b-4273-80d9-ad775ee0ad05"
      },
      "outputs": [],
      "source": [
        "# Launch training of the Unique Cycle GAN for center 0\n",
        "# Source domain = 0 / Target = test\n",
        "# No color augmentation is applied (values = 0)\n",
        "# Discriminator thresholding is enabled (D_thresh = 0.1)\n",
        "# Aberrant images from train/val are excluded\n",
        "\n",
        "!python -m CycleGAN.train_CycleGAN \\\n",
        "  --train_path \"/content/drive/MyDrive/kaggle-DL-MI/data/train.h5\" \\\n",
        "  --val_path \"/content/drive/MyDrive/kaggle-DL-MI/data/val.h5\" \\\n",
        "  --test_path \"/content/drive/MyDrive/kaggle-DL-MI/data/test.h5\" \\\n",
        "  --name test_run_domain_0 \\\n",
        "  --batch_size 64 \\\n",
        "  --gpu_ids 0 \\\n",
        "  --n_epochs 7 \\\n",
        "  --n_epochs_decay 3 \\\n",
        "  --lr_G 0.0002 \\\n",
        "  --lr_D 0.0002 \\\n",
        "  --save_epoch_freq 2 \\\n",
        "  --display_id 0 \\\n",
        "  --lambda_A 10.0 \\\n",
        "  --lambda_B 10.0 \\\n",
        "  --D_thresh \\\n",
        "  --D_thresh_value 0.1 \\\n",
        "  --lambda_identity 0.5 \\\n",
        "  --gan_mode lsgan \\\n",
        "  --domain 0 \\\n",
        "  --color_augment \\\n",
        "  --brightness 0.0 \\\n",
        "  --contrast 0.0 \\\n",
        "  --saturation 0.0 \\\n",
        "  --hue 0.0 \\\n",
        "  --aberrant_ids_train \"$str_aberrant_ids_train\" \\\n",
        "  --aberrant_ids_val \"$str_aberrant_ids_val\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bZjrneDNFeT",
        "outputId": "2fe93067-f899-47cb-80f8-2b970ae8551e"
      },
      "outputs": [],
      "source": [
        "# Center 3\n",
        "\n",
        "!python -m CycleGAN.train_CycleGAN \\\n",
        "  --train_path \"/content/drive/MyDrive/kaggle-DL-MI/data/train.h5\" \\\n",
        "  --val_path \"/content/drive/MyDrive/kaggle-DL-MI/data/val.h5\" \\\n",
        "  --test_path \"/content/drive/MyDrive/kaggle-DL-MI/data/test.h5\" \\\n",
        "  --name test_run_domain_3 \\\n",
        "  --batch_size 64 \\\n",
        "  --gpu_ids 0 \\\n",
        "  --n_epochs 7 \\\n",
        "  --n_epochs_decay 3 \\\n",
        "  --lr_G 0.0002 \\\n",
        "  --lr_D 0.0002 \\\n",
        "  --save_epoch_freq 2 \\\n",
        "  --display_id 0 \\\n",
        "  --lambda_A 10.0 \\\n",
        "  --lambda_B 10.0 \\\n",
        "  --D_thresh \\\n",
        "  --D_thresh_value 0.1 \\\n",
        "  --lambda_identity 0.5 \\\n",
        "  --gan_mode lsgan \\\n",
        "  --domain 3 \\\n",
        "  --color_augment \\\n",
        "  --brightness 0.0 \\\n",
        "  --contrast 0.0 \\\n",
        "  --saturation 0.0 \\\n",
        "  --hue 0.0 \\\n",
        "  --aberrant_ids_train \"$str_aberrant_ids_train\" \\\n",
        "  --aberrant_ids_val \"$str_aberrant_ids_val\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWjg8imCNUq9",
        "outputId": "f087fde3-a26e-4eef-a496-52764fd98b10"
      },
      "outputs": [],
      "source": [
        "# Center 4\n",
        "\n",
        "!python -m CycleGAN.train_CycleGAN \\\n",
        "  --train_path \"/content/drive/MyDrive/kaggle-DL-MI/data/train.h5\" \\\n",
        "  --val_path \"/content/drive/MyDrive/kaggle-DL-MI/data/val.h5\" \\\n",
        "  --test_path \"/content/drive/MyDrive/kaggle-DL-MI/data/test.h5\" \\\n",
        "  --name test_run_domain_4 \\\n",
        "  --batch_size 64 \\\n",
        "  --gpu_ids 0 \\\n",
        "  --n_epochs 7 \\\n",
        "  --n_epochs_decay 3 \\\n",
        "  --lr_G 0.0002 \\\n",
        "  --lr_D 0.0002 \\\n",
        "  --save_epoch_freq 2 \\\n",
        "  --display_id 0 \\\n",
        "  --lambda_A 10.0 \\\n",
        "  --lambda_B 10.0 \\\n",
        "  --D_thresh \\\n",
        "  --D_thresh_value 0.1 \\\n",
        "  --lambda_identity 0.5 \\\n",
        "  --gan_mode lsgan \\\n",
        "  --domain 4 \\\n",
        "  --color_augment \\\n",
        "  --brightness 0.0 \\\n",
        "  --contrast 0.0 \\\n",
        "  --saturation 0.0 \\\n",
        "  --hue 0.0 \\\n",
        "  --aberrant_ids_train \"$str_aberrant_ids_train\" \\\n",
        "  --aberrant_ids_val \"$str_aberrant_ids_val\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtOdjplKNcqa",
        "outputId": "42b4bb67-73cb-4256-e2ac-80f70787fb08"
      },
      "outputs": [],
      "source": [
        "# Center 1\n",
        "\n",
        "!python -m CycleGAN.train_CycleGAN \\\n",
        "  --train_path \"/content/drive/MyDrive/kaggle-DL-MI/data/train.h5\" \\\n",
        "  --val_path \"/content/drive/MyDrive/kaggle-DL-MI/data/val.h5\" \\\n",
        "  --test_path \"/content/drive/MyDrive/kaggle-DL-MI/data/test.h5\" \\\n",
        "  --name test_run_domain_1 \\\n",
        "  --batch_size 64 \\\n",
        "  --gpu_ids 0 \\\n",
        "  --n_epochs 7 \\\n",
        "  --n_epochs_decay 3 \\\n",
        "  --lr_G 0.0002 \\\n",
        "  --lr_D 0.0002 \\\n",
        "  --save_epoch_freq 2 \\\n",
        "  --display_id 0 \\\n",
        "  --lambda_A 10.0 \\\n",
        "  --lambda_B 10.0 \\\n",
        "  --D_thresh \\\n",
        "  --D_thresh_value 0.1 \\\n",
        "  --lambda_identity 0.5 \\\n",
        "  --gan_mode lsgan \\\n",
        "  --domain 1 \\\n",
        "  --color_augment \\\n",
        "  --brightness 0.0 \\\n",
        "  --contrast 0.0 \\\n",
        "  --saturation 0.0 \\\n",
        "  --hue 0.0 \\\n",
        "  --aberrant_ids_train \"$str_aberrant_ids_train\" \\\n",
        "  --aberrant_ids_val \"$str_aberrant_ids_val\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TuqxjSD9Nfox",
        "outputId": "4d356bbb-c50d-4552-8bce-311495d31ea3"
      },
      "outputs": [],
      "source": [
        "# Zip and download model checkpoints from Colab\n",
        "# Compress the 'checkpoints' directory (containing the 4 saved GAN weights)\n",
        "!zip -r /content/checkpoints_all_domains.zip /content/kaggle-DL-MI/checkpoints\n",
        "from google.colab import files\n",
        "files.download('/content/checkpoints_all_domains.zip')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GANs visualisation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "aKk4SHNd1w7T",
        "outputId": "0b38268e-d932-44ac-cbb1-236c91c41dd8"
      },
      "outputs": [],
      "source": [
        "# Load and visualize a specific image from train.h5 or val.h5\n",
        "# Converts the image if necessary to [C, H, W] format\n",
        "# Displays the image and prints metadata (center index, min/max values)\n",
        "\n",
        "import h5py\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load an image from val.h5 (domain A)\n",
        "h5_path = \"/content/drive/MyDrive/kaggle-DL-MI/data/val.h5\"\n",
        "index = 600  # Index of the image to inspect\n",
        "\n",
        "with h5py.File(h5_path, 'r') as f:\n",
        "    key = list(f.keys())[index]\n",
        "    img = torch.tensor(f[key]['img'][()])  # [H, W, C] or [3, H, W]\n",
        "    center_index = np.array(f[key]['metadata'])[0]\n",
        "\n",
        "# Convert to [C, H, W] if needed\n",
        "if img.ndim == 3 and img.shape[-1] == 3:\n",
        "    img = img.permute(2, 0, 1)\n",
        "    print(\"3 en dernier\")\n",
        "\n",
        "# Convert to float32 and [H, W, C] for matplotlib display\n",
        "img_np = img.permute(1, 2, 0).float().numpy()\n",
        "print(img_np.shape)\n",
        "print(np.max(img_np), np.min(img_np))\n",
        "print(center_index)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(img_np)\n",
        "plt.title(f\"Image {key}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P54OmUwjc8LM",
        "outputId": "d6cfeb16-7a58-408f-9224-733fc692e3e8"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained generators for image normalization (CycleGAN)\n",
        "# net_GA: multi-domain generator (MultiStain-CycleGAN trained on all centers)\n",
        "# gen_centerX: unique CycleGANs trained per center (0, 1, 3, 4)\n",
        "# Each generator is loaded from its corresponding checkpoint and set to eval mode\n",
        "# You need to import the weights downloaded before and place them in /content/kaggle-DL-MI/ with the right naming\n",
        "# Renaming the unique domain Cycle GANs weights is necessary ex : netG_A_0_epoch10.pth for center 0\n",
        "\n",
        "from CycleGAN import networks\n",
        "\n",
        "# Multi-domain generator (trained on all source domains → target domain)\n",
        "net_GA = networks.define_G(3, 3, 64, 'resnet_9blocks', 'instance', True, \"normal\", 0.02, [0])\n",
        "state_dict = torch.load(\"/content/kaggle-DL-MI/netG_A_epoch10.pth\", map_location='cpu')\n",
        "net_GA.load_state_dict(state_dict)\n",
        "net_GA.eval()\n",
        "\n",
        "# Unique CycleGAN generator for center 0\n",
        "gen_center0 = networks.define_G(3, 3, 64, 'resnet_9blocks', 'instance', True, \"normal\", 0.02, [0])\n",
        "state_dict0 = torch.load(\"/content/kaggle-DL-MI/netG_A_0_epoch10.pth\", map_location='cpu')\n",
        "gen_center0.load_state_dict(state_dict0)\n",
        "gen_center0.eval()\n",
        "\n",
        "# Unique CycleGAN generator for center 1\n",
        "gen_center1 = networks.define_G(3, 3, 64, 'resnet_9blocks', 'instance', True, \"normal\", 0.02, [0])\n",
        "state_dict1 = torch.load(\"/content/kaggle-DL-MI/netG_A_1_epoch10.pth\", map_location='cpu')\n",
        "gen_center1.load_state_dict(state_dict1)\n",
        "gen_center1.eval()\n",
        "\n",
        "# Unique CycleGAN generator for center 3\n",
        "gen_center3 = networks.define_G(3, 3, 64, 'resnet_9blocks', 'instance', True, \"normal\", 0.02, [0])\n",
        "state_dict3 = torch.load(\"/content/kaggle-DL-MI/netG_A_3_epoch10.pth\", map_location='cpu')\n",
        "gen_center3.load_state_dict(state_dict3)\n",
        "gen_center3.eval()\n",
        "\n",
        "# Unique CycleGAN generator for center 4\n",
        "gen_center4 = networks.define_G(3, 3, 64, 'resnet_9blocks', 'instance', True, \"normal\", 0.02, [0])\n",
        "state_dict4 = torch.load(\"/content/kaggle-DL-MI/netG_A_4_epoch10.pth\", map_location='cpu')\n",
        "gen_center4.load_state_dict(state_dict4)\n",
        "gen_center4.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "ZxDEqT_vewuN",
        "outputId": "eb8167e4-3f56-4931-8d7b-f74d9282a8d6"
      },
      "outputs": [],
      "source": [
        "# Apply MultiStain-CycleGAN generator (net_GA) to a sample image from domain A\n",
        "# The input image is first normalized to [-1, 1] before being passed through the generator\n",
        "# The output is then rescaled to [0, 255] and displayed with matplotlib\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Normalize image to [-1, 1] depending on its original scale\n",
        "if img.max() <= 1.0:\n",
        "    img = img.float() * 2.0 - 1.0\n",
        "    print(\"normal -1 1\")\n",
        "else:\n",
        "    img = img.float() / 127.5 - 1.0\n",
        "\n",
        "# Add batch dimension: [1, C, H, W]\n",
        "img_input = img.unsqueeze(0)\n",
        "\n",
        "# Apply the generator to translate the image to the target domain\n",
        "with torch.no_grad():\n",
        "    fake_B = net_GA(img_input)\n",
        "\n",
        "# Print output shape and value range\n",
        "print(\"fake_B shape:\", fake_B.shape)\n",
        "print(\"fake_B range: min =\", fake_B.min().item(), \", max =\", fake_B.max().item())\n",
        "\n",
        "# Convert the output to [H, W, C] in uint8 for visualization\n",
        "fake_B_np = ((fake_B.squeeze().cpu().numpy().transpose(1, 2, 0) + 1) / 2.0 * 255.0).astype(np.uint8)\n",
        "\n",
        "# Display the generated image\n",
        "plt.imshow(fake_B_np)\n",
        "plt.title(\"Image générée par Multi-Stain Cycle GAN\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "WZ9zDhfXTdcS",
        "outputId": "e60f68e2-4075-495c-9ccc-b6237bc1dcd8"
      },
      "outputs": [],
      "source": [
        "# Apply the unique CycleGAN generator corresponding to the image center\n",
        "# Normalize input image to [-1, 1] and apply the appropriate generator from the dictionary\n",
        "# Output image is rescaled to [0, 255] and displayed\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import torch\n",
        "\n",
        "# Dictionary containing trained generators for each center\n",
        "generators_dict = {\n",
        "    0: gen_center0,\n",
        "    1: gen_center1,\n",
        "    3: gen_center3,\n",
        "    4: gen_center4,\n",
        "}\n",
        "\n",
        "# Ensure image is in [C, H, W] format\n",
        "if img.ndim == 3 and img.shape[-1] == 3:\n",
        "    img = img.permute(2, 0, 1)\n",
        "\n",
        "# Normalize pixel values to [-1, 1]\n",
        "if img.max() <= 1.0:\n",
        "    img = img * 2.0 - 1.0\n",
        "    print(\"normalized [0, 1] → [-1, 1]\")\n",
        "else:\n",
        "    img = img / 127.5 - 1.0\n",
        "    print(\"normalized [0, 255] → [-1, 1]\")\n",
        "\n",
        "# Select the generator based on image center index\n",
        "generator = generators_dict.get(center_index)\n",
        "if generator is None:\n",
        "    raise ValueError(f\"No generator found for center {center_index}\")\n",
        "\n",
        "# Add batch dimension\n",
        "img_input = img.unsqueeze(0)\n",
        "\n",
        "# Generate normalized image with selected generator\n",
        "with torch.no_grad():\n",
        "    fake_B_unique = generator(img_input)\n",
        "\n",
        "# Output information\n",
        "print(\"fake_B shape:\", fake_B_unique.shape)\n",
        "print(\"fake_B range: min =\", fake_B_unique.min().item(), \", max =\", fake_B_unique.max().item())\n",
        "\n",
        "# Convert to [H, W, C] and scale to [0, 255] for display\n",
        "fake_B_np_unique = ((fake_B_unique.squeeze().cpu().numpy().transpose(1, 2, 0) + 1) / 2.0 * 255.0).astype(np.uint8)\n",
        "\n",
        "# Display the generated image\n",
        "plt.imshow(fake_B_np_unique)\n",
        "plt.title(f\"Generated image for center {center_index} using unique CycleGAN\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "KTtEn7ugj0MY",
        "outputId": "bc086557-fa57-45b5-f911-7bd8c670e75a"
      },
      "outputs": [],
      "source": [
        "# Display side-by-side comparison of:\n",
        "# (1) the original image from domain A (source center),\n",
        "# (2) the image translated by the Multi-domain CycleGAN,\n",
        "# (3) the image translated by the unique (per-center) CycleGAN\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
        "\n",
        "# Original image from source domain\n",
        "axes[0].imshow(img_np)\n",
        "axes[0].set_title(\"Original image (Domain A)\")\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "# Translated image using MultiStain-CycleGAN\n",
        "axes[1].imshow(fake_B_np)\n",
        "axes[1].set_title(\"Generated image (Domain B)\\nMulti-domain CycleGAN\")\n",
        "axes[1].axis(\"off\")\n",
        "\n",
        "# Translated image using unique per-center CycleGAN\n",
        "axes[2].imshow(fake_B_np_unique)\n",
        "axes[2].set_title(\"Generated image (Domain B)\\nUnique CycleGAN\")\n",
        "axes[2].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DinoV2 framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Features extraction test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxTxTvABwfZ5",
        "outputId": "de75a9c4-a08b-445c-fa1b-dbf32e7b3abc"
      },
      "outputs": [],
      "source": [
        "# Load the DINOv2 ViT-S/14 feature extractor from the official repository via torch.hub\n",
        "# The model is moved to GPU (\"cuda\") for faster inference\n",
        "feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3h2IijExrdD"
      },
      "outputs": [],
      "source": [
        "# Feature extraction from transformed image test\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "# Remove batch dimension → shape becomes [3, 96, 96]\n",
        "fake_B = fake_B.squeeze(0)\n",
        "\n",
        "# Rescale image pixel values from [-1, 1] to [0, 1]\n",
        "fake_B = (fake_B + 1) / 2.0\n",
        "\n",
        "# Resize image to 98x98 (required by DINOv2 which expects dimensions multiple of 14)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((98, 98))\n",
        "])\n",
        "fake_B = transform(fake_B)\n",
        "\n",
        "# Add back batch dimension → shape becomes [1, 3, 98, 98]\n",
        "fake_B = fake_B.unsqueeze(0)\n",
        "\n",
        "# Set the DINOv2 model to evaluation mode\n",
        "feature_extractor.eval()\n",
        "\n",
        "# Extract features without gradient tracking\n",
        "with torch.no_grad():\n",
        "    features = feature_extractor(fake_B)\n",
        "\n",
        "# Output feature shape (should be [1, 384] for ViT-S/14)\n",
        "features.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creation of Dataloaders "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6UpDeztz3U8"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "import h5py\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Define the IDs of aberrant images that should be excluded from training/validation\n",
        "aberrant_train_ids = list_aberrant_ids_train\n",
        "aberrant_val_ids = list_aberrant_ids_val\n",
        "\n",
        "# Data augmentation for training images (can help reduce overfitting)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    # transforms.ColorJitter(...)  # Optional: apply color jittering\n",
        "])\n",
        "\n",
        "# Custom dataset class that loads and optionally transforms GAN-normalized images\n",
        "class H5UnalignedDataset(Dataset):\n",
        "    def __init__(self, h5_path, transform=None, aberrant_ids_train=None, aberrant_ids_val=None,\n",
        "                 net_GA=None, generators=None, multi_gens=False, train=True):\n",
        "        super().__init__()\n",
        "        self.h5_path = h5_path\n",
        "        self.transform = transform\n",
        "        self.aberrant_ids_train = aberrant_ids_train or []\n",
        "        self.aberrant_ids_val = aberrant_ids_val or []\n",
        "        self.net_GA = net_GA  # Single GAN generator (MultiStain-CycleGAN)\n",
        "        self.generators = generators  # Dict of generators (one per center) if multi_gens=True\n",
        "        self.multi_gens = multi_gens\n",
        "        self.train = train\n",
        "\n",
        "        random.seed(42)\n",
        "\n",
        "        # Load all image keys from the h5 file\n",
        "        with h5py.File(self.h5_path, 'r') as f:\n",
        "            self.img_ids = list(f.keys())\n",
        "\n",
        "        # Exclude aberrant image IDs\n",
        "        if self.train:\n",
        "            self.img_ids = [img_id for img_id in self.img_ids if img_id not in self.aberrant_ids_train]\n",
        "        else:\n",
        "            self.img_ids = [img_id for img_id in self.img_ids if img_id not in self.aberrant_ids_val]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.img_ids[idx]\n",
        "        with h5py.File(self.h5_path, 'r') as f:\n",
        "            img = torch.tensor(f[img_id]['img'][()]).float()\n",
        "            label = np.array(f[img_id].get(\"label\"))  # label can be None for test set\n",
        "\n",
        "            # Extract center ID from metadata\n",
        "            metadata = f[img_id]['metadata']\n",
        "            center_id = int(np.array(metadata)[0])\n",
        "\n",
        "        # Apply data augmentation (if any)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Normalize to [-1, 1] before GAN input\n",
        "        img = img * 2.0 - 1.0\n",
        "\n",
        "        # Apply GAN generator (either single or by-center)\n",
        "        if self.multi_gens:\n",
        "            generator = self.generators.get(center_id)\n",
        "            if generator is not None:\n",
        "                img = generator(img.unsqueeze(0)).squeeze(0)\n",
        "            else:\n",
        "                raise ValueError(f\"No generator found for center {center_id}\")\n",
        "        elif self.net_GA is not None:\n",
        "            img = self.net_GA(img.unsqueeze(0)).squeeze(0)\n",
        "\n",
        "        # Rescale back to [0, 1]\n",
        "        img = (img + 1) / 2.0\n",
        "\n",
        "        return img, label\n",
        "\n",
        "# Create training dataset using MultiStain-CycleGAN here\n",
        "train_dataset = H5UnalignedDataset(\n",
        "    h5_path=\"/content/drive/MyDrive/kaggle-DL-MI/data/train.h5\",\n",
        "    transform=transform_train,\n",
        "    aberrant_ids_train=aberrant_train_ids,\n",
        "    aberrant_ids_val=aberrant_val_ids,\n",
        "    multi_gens=False,\n",
        "    #generators=generators_dict,\n",
        "    net_GA=net_GA,\n",
        "    train=True\n",
        ")\n",
        "\n",
        "# Create validation dataset (no augmentation)\n",
        "val_dataset = H5UnalignedDataset(\n",
        "    h5_path=\"/content/drive/MyDrive/kaggle-DL-MI/data/val.h5\",\n",
        "    transform=None,\n",
        "    aberrant_ids_train=aberrant_train_ids,\n",
        "    aberrant_ids_val=aberrant_val_ids,\n",
        "    multi_gens=False,\n",
        "    #generators=generators_dict,\n",
        "    net_GA=net_GA,\n",
        "    train=False\n",
        ")\n",
        "\n",
        "# Create DataLoaders for training and validation\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(len(train_dataset), len(val_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data visualization from datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "mt4pBWht11ww",
        "outputId": "02b5b2ba-7203-46c5-ee79-d1be85d36df6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to visualize a few images from a dataset (e.g., train or val)\n",
        "# The function retrieves 'num_images' samples and displays them in a row.\n",
        "def show_images_from_dataset(dataset, dataset_name=\"Train\", num_images=5):\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))  # Create a row of subplots\n",
        "    for i in range(num_images):\n",
        "        img, label = dataset[i]  # Get the image and its label\n",
        "\n",
        "        # Convert tensor image to numpy format [C,H,W] → [H,W,C]\n",
        "        img = img.detach().cpu().numpy().transpose((1, 2, 0))\n",
        "\n",
        "        # Plot image\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(f\"{dataset_name} Image {i+1} - Label: {label}\")\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.show()  # Display the figure\n",
        "\n",
        "# Display 5 images from the training and validation datasets\n",
        "show_images_from_dataset(train_dataset, \"Train\")\n",
        "show_images_from_dataset(val_dataset, \"Val\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fine-tunnig of DinoV2 and training + validation of classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "NZ5OdRKP2BR7",
        "outputId": "550dae6c-1d26-47fe-f653-37307dcf3918"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import h5py\n",
        "from torchmetrics.classification import BinaryAccuracy\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\"  # Use GPU if available\n",
        "\n",
        "# Resize images to 98x98 (required by DINOv2)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((98, 98)),\n",
        "])\n",
        "\n",
        "# Load the pre-trained DINOv2 model\n",
        "dino_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)\n",
        "\n",
        "# Freeze all layers except the last 2 transformer blocks + normalization layers\n",
        "for name, param in dino_model.named_parameters():\n",
        "    if not (\"blocks.10\" in name or \"blocks.11\" in name or \"norm\" in name or \"head\" in name):\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Define the MLP classifier used after DINOv2 features\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.fc4 = nn.Linear(128, output_size)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.fc4(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x.view(-1)\n",
        "\n",
        "feature_dim = 384  # DINOv2_vits14 output feature dimension\n",
        "\n",
        "# Instantiate the classifier\n",
        "classifier = Classifier(input_size=feature_dim, output_size=1).to(device)\n",
        "\n",
        "# Combine trainable DINO layers + classifier weights\n",
        "params_to_optimize = list(filter(lambda p: p.requires_grad, dino_model.parameters())) + list(classifier.parameters())\n",
        "\n",
        "# Training setup\n",
        "optimizer = optim.Adam(params_to_optimize, lr=1e-4, weight_decay=1e-5)\n",
        "criterion = nn.BCELoss()\n",
        "accuracy_metric = BinaryAccuracy().to(device)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "counter = 0\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    dino_model.train()\n",
        "    classifier.train()\n",
        "    total_loss, total_acc = 0, 0\n",
        "\n",
        "    for imgs, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
        "        imgs, labels = imgs.to(device), labels.float().to(device)\n",
        "        imgs = transform(imgs)\n",
        "\n",
        "        features = dino_model(imgs)\n",
        "        preds = classifier(features)\n",
        "\n",
        "        loss = criterion(preds, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        acc = accuracy_metric(preds > 0.5, labels.int())\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "        total_acc += acc.item() * imgs.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader.dataset)\n",
        "    avg_acc = total_acc / len(train_dataloader.dataset)\n",
        "    print(f\"Train Loss: {avg_loss:.4f} | Accuracy: {avg_acc:.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    dino_model.eval()\n",
        "    classifier.eval()\n",
        "    val_loss, val_acc = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(val_dataloader, desc=\"Val Phase\"):\n",
        "            imgs, labels = imgs.to(device), labels.float().to(device)\n",
        "            imgs = transform(imgs)\n",
        "\n",
        "            features = dino_model(imgs)\n",
        "            preds = classifier(features)\n",
        "\n",
        "            loss = criterion(preds, labels)\n",
        "            acc = accuracy_metric(preds > 0.5, labels.int())\n",
        "\n",
        "            val_loss += loss.item() * imgs.size(0)\n",
        "            val_acc += acc.item() * imgs.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader.dataset)\n",
        "    avg_val_acc = val_acc / len(val_dataloader.dataset)\n",
        "    print(f\"Val Loss: {avg_val_loss:.4f} | Accuracy: {avg_val_acc:.4f}\")\n",
        "\n",
        "    # Save best model based on validation loss\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        print(\"New best val_loss. Saving model.\")\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0\n",
        "        torch.save(classifier.state_dict(), \"best_classifier.pth\")\n",
        "        fine_tuned_dino_weights = {\n",
        "            k: v.cpu()\n",
        "            for k, v in dino_model.state_dict().items()\n",
        "            if any(layer in k for layer in [\"blocks.10\", \"blocks.11\", \"norm\", \"head\"])\n",
        "        }\n",
        "        torch.save(fine_tuned_dino_weights, \"best_finetuned_dino_layers.pth\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "hqUTd80LeKvw",
        "outputId": "363ac18a-f0fb-4617-8187-40607f307069"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "# Name of the zip archive to create\n",
        "zip_name = \"model_outputs.zip\"\n",
        "\n",
        "# Create the zip archive and add desired files\n",
        "with zipfile.ZipFile(zip_name, \"w\") as zipf:\n",
        "    zipf.write(\"best_classifier.pth\")               # Save the trained classifier\n",
        "    zipf.write(\"best_finetuned_dino_layers.pth\")     # Save the fine-tuned DINOv2 layers\n",
        "    # Add more files here if needed\n",
        "\n",
        "# Trigger download of the archive from Colab\n",
        "files.download(zip_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference on test set and submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9FlqpndcCGC",
        "outputId": "f9744646-2161-47f3-bf79-0480d9cba8e8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the trained classifier weights\n",
        "classifier.load_state_dict(torch.load(\"best_classifier.pth\", map_location=device))\n",
        "\n",
        "# Load fine-tuned DINOv2 weights (only partial layers)\n",
        "finetuned_weights = torch.load(\"best_finetuned_dino_layers.pth\", map_location=device)\n",
        "\n",
        "# Merge fine-tuned layers into the full DINOv2 state_dict\n",
        "state_dict = dino_model.state_dict()\n",
        "state_dict.update(finetuned_weights)  # Update blocks.10, blocks.11, norm, head\n",
        "dino_model.load_state_dict(state_dict)\n",
        "\n",
        "# Set models to evaluation mode\n",
        "classifier.eval()\n",
        "dino_model.eval()\n",
        "\n",
        "# Prepare the dictionary to store predictions\n",
        "solutions_data = {'ID': [], 'Pred': []}\n",
        "\n",
        "# Load and process each image from the test set\n",
        "with h5py.File(\"/content/drive/MyDrive/kaggle-DL-MI/data/test.h5\", 'r') as hdf:\n",
        "    test_ids = list(hdf.keys())\n",
        "\n",
        "    for test_id in tqdm(test_ids):\n",
        "        # Load the test image as a tensor\n",
        "        img = torch.tensor(np.array(hdf.get(test_id).get('img'))).float()\n",
        "\n",
        "        # Resize the image to match DINOv2 input constraints\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((98, 98))\n",
        "        ])\n",
        "        img_resized = transform(img)\n",
        "\n",
        "        # Extract DINOv2 features\n",
        "        with torch.no_grad():\n",
        "            features = dino_model(img_resized.unsqueeze(0).to('cuda')).squeeze(0)\n",
        "\n",
        "        # Run classifier to get prediction\n",
        "        pred = classifier(features.unsqueeze(0)).detach().cpu()\n",
        "\n",
        "        # Save binary prediction (threshold at 0.5)\n",
        "        solutions_data['ID'].append(int(test_id))\n",
        "        solutions_data['Pred'].append(int(pred.item() > 0.5))\n",
        "\n",
        "# Save results to CSV file\n",
        "solutions_data = pd.DataFrame(solutions_data).set_index('ID')\n",
        "solutions_data.to_csv('cycleGAN_fine_tune_dino_submit.csv')\n",
        "\n",
        "print(\"Submission saved to 'cycleGAN_fine_tune_dino_submit.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "hlAxJ_NTghlx",
        "outputId": "96eeec7d-ca34-4e40-e4ab-4dca4d4ef5da"
      },
      "outputs": [],
      "source": [
        "# Download submission\n",
        "from google.colab import files\n",
        "files.download(\"cycleGAN_fine_tune_dino_submit.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Optionnal) Train of train+val datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjuAYWpV99Ey"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import ConcatDataset, DataLoader\n",
        "\n",
        "# Create a \"validation-as-training\" dataset using the validation set\n",
        "# This version applies data augmentation (transform_train) and domain adaptation (GAN)\n",
        "val_dataset_train = H5UnalignedDataset(\n",
        "    h5_path=\"/content/drive/MyDrive/kaggle-DL-MI/data/val.h5\",\n",
        "    transform=transform_train,                     # Apply the same augmentation as for training\n",
        "    aberrant_ids_train=aberrant_train_ids,\n",
        "    aberrant_ids_val=aberrant_val_ids,\n",
        "    multi_gens=False,\n",
        "    #generators=generators_dict,\n",
        "    net_GA = net_GA,\n",
        "    train=False\n",
        ")\n",
        "\n",
        "# Merge train and val datasets to form a combined training set\n",
        "combined_dataset = ConcatDataset([train_dataset, val_dataset_train])\n",
        "\n",
        "# Create DataLoader from the merged dataset\n",
        "combined_dataloader = DataLoader(combined_dataset, batch_size=256, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQ9aHE8i-RAw",
        "outputId": "12710d78-6e68-4065-f011-481db415b301"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import h5py\n",
        "from torchmetrics.classification import BinaryAccuracy\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\"\n",
        "\n",
        "# Resize images to 98x98 (required for DINOv2)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((98, 98)),\n",
        "])\n",
        "\n",
        "# Load pre-trained DINOv2 model\n",
        "dino_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)\n",
        "\n",
        "# Freeze all layers except the last 2 transformer blocks, normalization, and the head\n",
        "for name, param in dino_model.named_parameters():\n",
        "    if not (\"blocks.10\" in name or \"blocks.11\" in name or \"norm\" in name or \"head\" in name):\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Define the classifier head\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.fc4 = nn.Linear(128, output_size)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.fc4(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x.view(-1)  # Ensure output shape = (batch_size,)\n",
        "\n",
        "# Input feature size from DINOv2\n",
        "feature_dim = 384\n",
        "\n",
        "# Instantiate the classifier and move to device\n",
        "classifier = Classifier(input_size=feature_dim, output_size=1).to(device)\n",
        "\n",
        "# Combine trainable DINO layers and classifier parameters\n",
        "params_to_optimize = list(filter(lambda p: p.requires_grad, dino_model.parameters())) + list(classifier.parameters())\n",
        "\n",
        "# Define optimizer with weight decay\n",
        "optimizer = optim.Adam(params_to_optimize, lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "# Binary cross-entropy loss and accuracy metric\n",
        "criterion = nn.BCELoss()\n",
        "accuracy_metric = BinaryAccuracy().to(device)\n",
        "\n",
        "# Number of training epochs\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    dino_model.train()\n",
        "    classifier.train()\n",
        "    total_loss, total_acc = 0, 0\n",
        "\n",
        "    for imgs, labels in tqdm(combined_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
        "        imgs, labels = imgs.to(device), labels.float().to(device)\n",
        "        imgs = transform(imgs)  # Resize to 98x98\n",
        "\n",
        "        features = dino_model(imgs)\n",
        "        preds = classifier(features)\n",
        "\n",
        "        loss = criterion(preds, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        acc = accuracy_metric(preds > 0.5, labels.int())\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "        total_acc += acc.item() * imgs.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(combined_dataloader.dataset)\n",
        "    avg_acc = total_acc / len(combined_dataloader.dataset)\n",
        "    print(f\"Train Loss: {avg_loss:.4f} | Accuracy: {avg_acc:.4f}\")\n",
        "\n",
        "# Save the classifier weights\n",
        "torch.save(classifier.state_dict(), \"best_classifier_full.pth\")\n",
        "\n",
        "# Save only the fine-tuned layers of DINOv2\n",
        "fine_tuned_dino_weights = {\n",
        "    k: v.cpu()\n",
        "    for k, v in dino_model.state_dict().items()\n",
        "    if any(layer in k for layer in [\"blocks.10\", \"blocks.11\", \"norm\", \"head\"])\n",
        "}\n",
        "torch.save(fine_tuned_dino_weights, \"best_finetuned_dino_layers_full.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "VftEivcb8nJI",
        "outputId": "bc3b1111-41f9-49a0-9ad2-720a85b85b41"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "# Create a zip archive containing the full model outputs\n",
        "zip_name = \"model_full_outputs.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_name, \"w\") as zipf:\n",
        "    zipf.write(\"best_classifier_full.pth\")             # Add the trained classifier weights\n",
        "    zipf.write(\"best_finetuned_dino_layers_full.pth\")  # Add the fine-tuned DINOv2 layers\n",
        "\n",
        "# Trigger the download in the Colab interface\n",
        "files.download(zip_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdfoH26jrUZK",
        "outputId": "45ef2d68-636a-49a9-8494-87040b2528e7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the classifier trained on train + val\n",
        "classifier = Classifier(input_size=feature_dim, output_size=1).to('cuda')\n",
        "classifier.load_state_dict(torch.load(\"best_classifier_full.pth\", map_location=device))\n",
        "\n",
        "# Load the fine-tuned DINOv2 layers\n",
        "finetuned_weights = torch.load(\"best_finetuned_dino_layers_full.pth\", map_location=device)\n",
        "\n",
        "# Update only the relevant layers in the DINOv2 model\n",
        "state_dict = dino_model.state_dict()\n",
        "state_dict.update(finetuned_weights)  # Replace blocks 10, 11, norm, and head\n",
        "dino_model.load_state_dict(state_dict)\n",
        "\n",
        "# Set both models to evaluation mode\n",
        "classifier.eval()\n",
        "dino_model.eval()\n",
        "\n",
        "# Prepare dictionary to store predictions\n",
        "solutions_data = {'ID': [], 'Pred': []}\n",
        "\n",
        "# Load test images and run inference\n",
        "with h5py.File(\"/content/drive/MyDrive/kaggle-DL-MI/data/test.h5\", 'r') as hdf:\n",
        "    test_ids = list(hdf.keys())\n",
        "\n",
        "    for test_id in tqdm(test_ids):\n",
        "        # Load the image (already normalized to test domain)\n",
        "        img = torch.tensor(np.array(hdf.get(test_id).get('img'))).float()\n",
        "\n",
        "        # Resize to match DINOv2 input size\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((98, 98)),\n",
        "        ])\n",
        "        img_resized = transform(img)\n",
        "\n",
        "        # Extract features using DINOv2\n",
        "        with torch.no_grad():\n",
        "            features = dino_model(img_resized.unsqueeze(0).to('cuda')).squeeze(0)\n",
        "\n",
        "        # Predict using the classifier\n",
        "        pred = classifier(features.unsqueeze(0)).detach().cpu()\n",
        "\n",
        "        # Store prediction as binary label\n",
        "        solutions_data['ID'].append(int(test_id))\n",
        "        solutions_data['Pred'].append(int(pred.item() > 0.5))\n",
        "\n",
        "# Save results to CSV for Kaggle submission\n",
        "solutions_data = pd.DataFrame(solutions_data).set_index('ID')\n",
        "solutions_data.to_csv('cycleGAN_fine_tune_dino_submit_full.csv')\n",
        "\n",
        "print(\"Submission saved to 'cycleGAN_fine_tune_dino_submit_full.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "LFvIAr9HvHGi",
        "outputId": "c1cb9cf1-331e-48e5-e6bb-1d994626b683"
      },
      "outputs": [],
      "source": [
        "# Download submission\n",
        "from google.colab import files\n",
        "files.download('cycleGAN_fine_tune_dino_submit_full.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WvNTjIy0_tH"
      },
      "source": [
        "## MedImageInsight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recquired downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfm_h-7D05dV",
        "outputId": "5e3e84fc-5a3d-49c3-933e-ad0463bfe24d"
      },
      "outputs": [],
      "source": [
        "!git lfs install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AJ8whznLQQn",
        "outputId": "35553e1f-ca64-4c41-ebdf-ebcb49528cd4"
      },
      "outputs": [],
      "source": [
        "%cd kaggle-DL-MI/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouSskl_b0_Ko",
        "outputId": "be51f4c3-30d0-45e6-cf22-b96d7dd5b183"
      },
      "outputs": [],
      "source": [
        "!git clone https://huggingface.co/lion-ai/MedImageInsights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbrgROmQ1Jcr",
        "outputId": "8a41bec5-cdd3-4f11-db60-58362e9ddab3"
      },
      "outputs": [],
      "source": [
        "%cd MedImageInsights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nhyBglgBkAZ",
        "outputId": "daa007ab-d9d9-4de9-cea6-690636e1d80d"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF7jnYxj1MhA",
        "outputId": "2cd3a38a-fd91-44d6-8fc9-aa136e6bfad9"
      },
      "outputs": [],
      "source": [
        "!uv sync"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8XxqiKo1OA0",
        "outputId": "fa6481aa-f08b-4eb4-f111-4db8eff947cb"
      },
      "outputs": [],
      "source": [
        "!pip install mup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sep3OYa1TPh",
        "outputId": "5bce740f-70fc-4e83-df6c-c0fdc1dd7bd8"
      },
      "outputs": [],
      "source": [
        "!pip install fvcore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbeRHq_A1VWU",
        "outputId": "c07f3c03-3091-487e-83eb-29beddf8b408"
      },
      "outputs": [],
      "source": [
        "from medimageinsightmodel import MedImageInsight\n",
        "\n",
        "# Initialize the MedImageInsight model\n",
        "# The model uses a vision encoder pre-trained specifically for medical imaging\n",
        "embedding_extractor = MedImageInsight(\n",
        "    model_dir=\"2024.09.27\",                         # Directory containing the model files\n",
        "    vision_model_name=\"medimageinsigt-v1.0.0.pt\",   # Vision encoder checkpoint\n",
        "    language_model_name=\"language_model.pth\"        # Language encoder checkpoint (not used here)\n",
        ")\n",
        "\n",
        "# Load the vision encoder weights (and text encoder if needed)\n",
        "embedding_extractor.load_model()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embeddings extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epJL8DfE3WXB"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4lP38UB1nr2"
      },
      "outputs": [],
      "source": [
        "import base64, io\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_embeddings_from_dataloader(dataloader, model, save_path=None, max_batches=None):\n",
        "    \"\"\"\n",
        "    Extracts image embeddings from a dataloader using a base64-based model API (e.g., MedImageInsight).\n",
        "\n",
        "    Args:\n",
        "        dataloader: PyTorch DataLoader containing images and labels.\n",
        "        model: Feature extractor with a .encode(images=[base64 strings]) method.\n",
        "        save_path: Optional path to save the output DataFrame as a .pkl file.\n",
        "        max_batches: Optional number of batches to process (useful for debugging or speed constraints).\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame with columns ['ID', 'label', 'embedding'].\n",
        "    \"\"\"\n",
        "\n",
        "    image_ids, labels, all_embeddings = [], [], []\n",
        "\n",
        "    for batch_idx, (imgs, lbls) in enumerate(tqdm(dataloader, desc=\"Extraction d'embeddings\")):\n",
        "        if max_batches is not None and batch_idx >= max_batches:\n",
        "            break\n",
        "\n",
        "        batch_b64 = []\n",
        "        for img in imgs:\n",
        "            # Convert image from [C, H, W] to [H, W, C] and move to CPU\n",
        "            np_img = img.permute(1, 2, 0).detach().cpu().numpy()\n",
        "\n",
        "            # Rescale to [0, 255] and convert to uint8 if needed\n",
        "            if np_img.max() <= 1.0:\n",
        "                np_img = (np_img * 255).astype(np.uint8)\n",
        "            else:\n",
        "                np_img = np_img.astype(np.uint8)\n",
        "\n",
        "            # Encode the image as base64 PNG for model input\n",
        "            buffer = io.BytesIO()\n",
        "            Image.fromarray(np_img).save(buffer, format=\"PNG\")\n",
        "            img_b64 = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "            batch_b64.append(img_b64)\n",
        "\n",
        "        try:\n",
        "            # Send batch of base64 images to the model\n",
        "            result = model.encode(images=batch_b64)\n",
        "            embeddings = result[\"image_embeddings\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Error while encoding batch {batch_idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "        all_embeddings.extend(embeddings)\n",
        "        labels.extend(lbls.tolist())\n",
        "        image_ids.extend([f\"batch{batch_idx}_img{i}\" for i in range(len(lbls))])  # Temporary ID\n",
        "\n",
        "    # Create dataframe with extracted embeddings and labels\n",
        "    df_embed = pd.DataFrame({\n",
        "        \"ID\": image_ids,\n",
        "        \"label\": labels,\n",
        "        \"embedding\": all_embeddings\n",
        "    })\n",
        "\n",
        "    if save_path:\n",
        "        df_embed.to_pickle(save_path)\n",
        "        print(f\"Embeddings saved to {save_path}\")\n",
        "\n",
        "    return df_embed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-2jI7MI4LnO",
        "outputId": "d41ede6f-c297-4d26-e9f9-cd1dcac5df8e"
      },
      "outputs": [],
      "source": [
        "df_train_embed = extract_embeddings_from_dataloader(train_dataloader, embedding_extractor, save_path=\"train_GAN_embed.pkl\")\n",
        "df_val_embed = extract_embeddings_from_dataloader(val_dataloader, embedding_extractor, save_path=\"val_GAN_embed.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "FoB7DLdM54OT",
        "outputId": "c2ce01b7-b9b0-4b08-aae9-21852d5e32f3"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "# Define the name of the output zip file\n",
        "zip_name = \"embeds_train_val.zip\"\n",
        "\n",
        "# Create a zip archive containing the train and val embedding files\n",
        "with zipfile.ZipFile(zip_name, \"w\") as zipf:\n",
        "    zipf.write(\"train_GAN_embed.pkl\")\n",
        "    zipf.write(\"val_GAN_embed.pkl\")\n",
        "    # Add more files here if needed\n",
        "\n",
        "# Trigger download of the zip archive\n",
        "files.download(zip_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEG13x9t6In3"
      },
      "outputs": [],
      "source": [
        "def extract_embeddings_from_h5_batch(h5_path, model, max_images=None, batch_size=64, save_path=None):\n",
        "    \"\"\"\n",
        "    Extract image embeddings from a .h5 dataset using a model that supports base64 input.\n",
        "\n",
        "    Parameters:\n",
        "    - h5_path (str): Path to the HDF5 file.\n",
        "    - model: Model object with an 'encode(images=...)' method.\n",
        "    - max_images (int): Optional limit on number of images to process.\n",
        "    - batch_size (int): Number of images per batch sent to the model.\n",
        "    - save_path (str): Optional path to save resulting DataFrame as .pkl file.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: DataFrame containing image IDs, labels, and embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    image_ids, labels, all_embeddings = [], [], []\n",
        "    batch_b64, batch_ids, batch_labels = [], [], []\n",
        "\n",
        "    with h5py.File(h5_path, 'r') as hdf:\n",
        "        ids = list(hdf.keys())\n",
        "        if max_images is not None:\n",
        "            ids = ids[:max_images]\n",
        "\n",
        "        for i, img_id in enumerate(tqdm(ids)):\n",
        "            # Load image array\n",
        "            img_array = np.array(hdf[img_id]['img'])\n",
        "\n",
        "            # Ensure correct shape: [H, W, C]\n",
        "            if img_array.shape[0] == 3:\n",
        "                img_array = np.transpose(img_array, (1, 2, 0))\n",
        "\n",
        "            # Convert to uint8 in [0, 255]\n",
        "            if img_array.max() <= 1.0:\n",
        "                img_array = (img_array * 255).astype(np.uint8)\n",
        "            else:\n",
        "                img_array = img_array.astype(np.uint8)\n",
        "\n",
        "            # Encode image to base64\n",
        "            buffer = io.BytesIO()\n",
        "            Image.fromarray(img_array).save(buffer, format=\"PNG\")\n",
        "            img_b64 = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "            # Accumulate batch info\n",
        "            batch_b64.append(img_b64)\n",
        "            batch_ids.append(img_id)\n",
        "            batch_labels.append(int(np.array(hdf[img_id]['label'])) if 'label' in hdf[img_id] else None)\n",
        "\n",
        "            # Once batch is full or last image → send to model\n",
        "            if len(batch_b64) == batch_size or (i == len(ids) - 1):\n",
        "                try:\n",
        "                    result = model.encode(images=batch_b64)\n",
        "                    embeddings = result[\"image_embeddings\"]\n",
        "                except Exception as e:\n",
        "                    print(f\"Error at batch {i}: {e}\")\n",
        "                    batch_b64, batch_ids, batch_labels = [], [], []\n",
        "                    continue\n",
        "\n",
        "                all_embeddings.extend(embeddings)\n",
        "                image_ids.extend(batch_ids)\n",
        "                labels.extend(batch_labels)\n",
        "\n",
        "                # Reset batch\n",
        "                batch_b64, batch_ids, batch_labels = [], [], []\n",
        "                print(\"Batch processed.\")\n",
        "\n",
        "    # Build resulting DataFrame\n",
        "    df_embed = pd.DataFrame({\n",
        "        \"ID\": image_ids,\n",
        "        \"label\": labels,\n",
        "        \"embedding\": all_embeddings\n",
        "    })\n",
        "\n",
        "    if save_path:\n",
        "        df_embed.to_pickle(save_path)\n",
        "        print(f\"✅ Embeddings saved to {save_path}\")\n",
        "\n",
        "    return df_embed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih9EGRJj6N4e",
        "outputId": "d0e42e93-7b05-4066-e25d-8495b0816ae1"
      },
      "outputs": [],
      "source": [
        "df_test_embed = extract_embeddings_from_h5_batch(\n",
        "    h5_path=\"/content/drive/MyDrive/kaggle-DL-MI/data/test.h5\",\n",
        "    model=embedding_extractor,\n",
        "    batch_size=64,\n",
        "    save_path=\"test_embed.pkl\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Bap6bpJM6V-2",
        "outputId": "ae33e62f-48c2-423c-ca80-ad9f0738edf8"
      },
      "outputs": [],
      "source": [
        "# Download test embeddings\n",
        "\n",
        "from google.colab import files\n",
        "files.download('test_embed.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training classifier with training embeddings and validation on val embeddings "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "071-vbowwh9b",
        "outputId": "b50682dd-451e-4e03-c1a0-45b4cf373122"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchmetrics.classification import BinaryAccuracy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Define MLP classifier for binary prediction from embeddings\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.fc4 = nn.Linear(128, output_size)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.fc4(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x.view(-1)  # Output shape: (batch_size,)\n",
        "\n",
        "# Load training and validation embedding datasets\n",
        "train_embedd_dataset = pd.read_pickle(\"train_GAN_embed.pkl\")\n",
        "val_embedd_dataset = pd.read_pickle(\"val_GAN_embed.pkl\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Custom Dataset to handle embeddings and labels\n",
        "class EmbeddingDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.embeddings = dataframe[\"embedding\"].tolist()\n",
        "        self.labels = dataframe[\"label\"].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding = torch.tensor(self.embeddings[idx]).float()\n",
        "        label = torch.tensor(self.labels[idx]).float()\n",
        "        return embedding, label\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = EmbeddingDataset(train_embedd_dataset)\n",
        "val_dataset = EmbeddingDataset(val_embedd_dataset)\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "# Initialize model and training components\n",
        "feature_dim = len(train_embedd_dataset.iloc[0][\"embedding\"])\n",
        "classifier = Classifier(input_size=feature_dim, output_size=1).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "metric = BinaryAccuracy().to(device)\n",
        "\n",
        "# Add learning rate scheduler to reduce LR on plateau\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        ")\n",
        "\n",
        "# Training loop with early stopping\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "counter = 0\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    classifier.train()\n",
        "    total_loss, total_acc = 0, 0\n",
        "\n",
        "    for emb, lbl in tqdm(train_loader, desc=f\"[Epoch {epoch+1}]\"):\n",
        "        emb, lbl = emb.to(device), lbl.to(device)\n",
        "        preds = classifier(emb)\n",
        "        loss = criterion(preds, lbl)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        acc = metric(preds > 0.5, lbl.int())\n",
        "        total_loss += loss.item() * emb.size(0)\n",
        "        total_acc += acc.item() * emb.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "    avg_acc = total_acc / len(train_loader.dataset)\n",
        "    print(f\"Train Loss: {avg_loss:.4f} | Accuracy: {avg_acc:.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    classifier.eval()\n",
        "    val_loss, val_acc = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for emb, lbl in val_loader:\n",
        "            emb, lbl = emb.to(device), lbl.to(device)\n",
        "            preds = classifier(emb)\n",
        "            loss = criterion(preds, lbl)\n",
        "            acc = metric(preds > 0.5, lbl.int())\n",
        "\n",
        "            val_loss += loss.item() * emb.size(0)\n",
        "            val_acc += acc.item() * emb.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "    avg_val_acc = val_acc / len(val_loader.dataset)\n",
        "    print(f\"Val Loss: {avg_val_loss:.4f} | Accuracy: {avg_val_acc:.4f}\")\n",
        "\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    # Save best model and check early stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        print(\"New best validation loss. Saving model.\")\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0\n",
        "        torch.save(classifier.state_dict(), \"best_embed_classifier.pth\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NBl3zkCLykeG",
        "outputId": "8b5dbd56-39ba-4b9f-a056-32208bbb77b4"
      },
      "outputs": [],
      "source": [
        "# Download classifier weights\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "files.download(\"best_embed_classifier.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference on test embeddings and submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYoDBbj3w0Or"
      },
      "outputs": [],
      "source": [
        "test_embedd_dataset = pd.read_pickle(\"test_embed.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n30G0HlZyNhm",
        "outputId": "83190d8e-dfaa-415d-fa42-40d56a98d1fe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Custom Dataset for test embeddings\n",
        "class EmbeddingTestDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.embeddings = dataframe[\"embedding\"].tolist()\n",
        "        self.ids = dataframe[\"ID\"].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding = torch.tensor(self.embeddings[idx]).float()\n",
        "        id_ = self.ids[idx]\n",
        "        return embedding, id_\n",
        "\n",
        "# Load trained classifier\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "feature_dim = len(test_embedd_dataset.iloc[0][\"embedding\"])\n",
        "\n",
        "classifier = Classifier(input_size=feature_dim, output_size=1).to(device)\n",
        "classifier.load_state_dict(torch.load(\"best_embed_classifier.pth\", map_location=device))\n",
        "classifier.eval()\n",
        "\n",
        "# Create DataLoader for test embeddings\n",
        "test_dataset = EmbeddingTestDataset(test_embedd_dataset)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "# Perform inference\n",
        "ids, probs, preds_bin = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for emb, id_ in tqdm(test_loader, desc=\"Inference\"):\n",
        "        emb = emb.to(device)\n",
        "        prob = classifier(emb)  # Predicted probability\n",
        "        pred = (prob > 0.5).int()  # Binary prediction\n",
        "\n",
        "        ids.extend(id_)\n",
        "        probs.extend(prob.cpu().numpy().tolist())\n",
        "        preds_bin.extend(pred.cpu().numpy().tolist())\n",
        "\n",
        "# Create final submission DataFrame\n",
        "df_submission = pd.DataFrame({\n",
        "    \"ID\": ids,\n",
        "    \"Pred\": preds_bin\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqtrFOt6zpY6",
        "outputId": "fc0092da-153d-42ab-a8b2-7e25e7ce80cc"
      },
      "outputs": [],
      "source": [
        "# Save the submission file as CSV\n",
        "df_submission.to_csv(\"submission_MedImg_GAN.csv\", index=False)\n",
        "print(\"submission_MedImg_GAN.csv generated with probabilities and binary predictions!\")\n",
        "\n",
        "# Download the CSV file from Colab\n",
        "from google.colab import files\n",
        "files.download(\"submission_MedImg_GAN.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Optionnal) Train+val training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWa9hRuUABzP"
      },
      "outputs": [],
      "source": [
        "# Combine the training and validation datasets into a single dataset\n",
        "combined_dataset = ConcatDataset([train_dataset, val_dataset])\n",
        "\n",
        "# Create a DataLoader for the combined dataset (used for final training)\n",
        "combined_dataloader = DataLoader(combined_dataset, batch_size=256, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFrXRccwTS_Z",
        "outputId": "cd4c6311-aaa3-4666-eabd-4aa5fa8062fd"
      },
      "outputs": [],
      "source": [
        "# Initialize the classifier with the appropriate input size (matching the embedding dimension)\n",
        "feature_dim = len(train_embedd_dataset.iloc[0][\"embedding\"])\n",
        "classifier = Classifier(input_size=feature_dim, output_size=1).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "metric = BinaryAccuracy().to(device)\n",
        "\n",
        "# Training loop parameters\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "counter = 0\n",
        "NUM_EPOCHS = 15\n",
        "\n",
        "# Training loop over combined training + validation embeddings\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    classifier.train()\n",
        "    total_loss, total_acc = 0, 0\n",
        "\n",
        "    for emb, lbl in tqdm(combined_dataloader, desc=f\"[Epoch {epoch+1}]\"):\n",
        "        emb, lbl = emb.to(device), lbl.to(device)\n",
        "\n",
        "        preds = classifier(emb)\n",
        "        loss = criterion(preds, lbl)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        acc = metric(preds > 0.5, lbl.int())\n",
        "        total_loss += loss.item() * emb.size(0)\n",
        "        total_acc += acc.item() * emb.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(combined_dataloader.dataset)\n",
        "    avg_acc = total_acc / len(combined_dataloader.dataset)\n",
        "    print(f\"Train Loss: {avg_loss:.4f} | Accuracy: {avg_acc:.4f}\")\n",
        "\n",
        "# Save the trained classifier weights\n",
        "torch.save(classifier.state_dict(), \"best_embed_classifier_full.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFuoTCNuYqtU",
        "outputId": "78d49ae1-dae4-42e6-f573-04fee27336ba"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Custom dataset for test embeddings\n",
        "class EmbeddingTestDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.embeddings = dataframe[\"embedding\"].tolist()\n",
        "        self.ids = dataframe[\"ID\"].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding = torch.tensor(self.embeddings[idx]).float()\n",
        "        id_ = self.ids[idx]\n",
        "        return embedding, id_\n",
        "\n",
        "# Load trained classifier\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "feature_dim = len(test_embedd_dataset.iloc[0][\"embedding\"])\n",
        "\n",
        "classifier = Classifier(input_size=feature_dim, output_size=1).to(device)\n",
        "classifier.load_state_dict(torch.load(\"best_embed_classifier_full.pth\", map_location=device))\n",
        "classifier.eval()\n",
        "\n",
        "# Create test DataLoader\n",
        "test_dataset = EmbeddingTestDataset(test_embedd_dataset)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "# Inference loop\n",
        "ids, probs, preds_bin = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for emb, id_ in tqdm(test_loader, desc=\"Inference\"):\n",
        "        emb = emb.to(device)\n",
        "        prob = classifier(emb)\n",
        "        pred = (prob > 0.5).int()  # Convert probabilities to binary predictions\n",
        "\n",
        "        ids.extend(id_)\n",
        "        probs.extend(prob.cpu().numpy().tolist())\n",
        "        preds_bin.extend(pred.cpu().numpy().tolist())\n",
        "\n",
        "# Create the submission DataFrame\n",
        "df_submission = pd.DataFrame({\n",
        "    \"ID\": ids,\n",
        "    \"Pred\": preds_bin\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ND30SoS1Yv7E",
        "outputId": "8e971f3f-1aef-4105-c3c5-a8793d75b17d"
      },
      "outputs": [],
      "source": [
        "# Save the submission DataFrame to a CSV file\n",
        "df_submission.to_csv(\"submission_MedImg_GAN_full.csv\", index=False)\n",
        "print(\"submission_MedImg_GAN_full.csv has been saved with binary predictions.\")\n",
        "\n",
        "# Download the CSV file to your local machine\n",
        "from google.colab import files\n",
        "files.download(\"submission_MedImg_GAN_full.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lewagon",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
